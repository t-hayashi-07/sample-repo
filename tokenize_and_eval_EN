#!/usr/bin/env python3
"""
Task 1 Minimal Setup: Shakespeare Text Splitting and Subword Evaluation
"""

import os
import csv
import random
from typing import List, Tuple
import sentencepiece as spm


def split_data(input_file: str, train_ratio: float = 0.99) -> Tuple[str, str]:
    """
    Split data into training and test sets
    
    Purpose: Divide Shakespeare text into 99% (training) and 1% (evaluation)
    Why needed: To measure performance on unseen data (prevent overfitting)
    """
    print(f"📖 Reading {input_file}...")
    
    # Read Shakespeare text file line by line
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()  # Store all lines in list
    
    print(f"   Total lines read: {len(lines)}")
    
    # Randomly shuffle lines (to prevent bias)
    # Example: Avoid first 99% being all dialogue, last 1% being all stage directions
    random.shuffle(lines)
    
    # Calculate split point (e.g., if 1000 lines: 990 for training, 10 for evaluation)
    split_point = int(len(lines) * train_ratio)
    print(f"   Split at line {split_point} (train: {train_ratio*100}%)")
    
    # Split list using slicing
    train_lines = lines[:split_point]      # Lines 0-989
    test_lines = lines[split_point:]       # Lines 990-999
    
    # Save split data to separate files
    train_file = 'train.txt'
    test_file = 'test.txt'
    
    # Create training file (SentencePiece will read this for model training)
    with open(train_file, 'w', encoding='utf-8') as f:
        f.writelines(train_lines)
    
    # Create evaluation file (used for trained model performance measurement)
    with open(test_file, 'w', encoding='utf-8') as f:
        f.writelines(test_lines)
    
    print(f"✅ Split complete: {len(train_lines)} train, {len(test_lines)} test lines")
    print(f"   Created: {train_file}, {test_file}")
    return train_file, test_file


def train_sentencepiece_models(train_file: str, vocab_sizes: List[int]):
    """
    Train SentencePiece models with multiple vocabulary sizes
    
    Purpose: Create 3 BPE models with different vocabulary sizes
    Example: Learn how to split text with 500, 1000, 5000 pieces
    """
    print("🤖 Training SentencePiece models...")
    print("   What's happening: BPE algorithm finds the best way to split text")
    print("   into reusable pieces (subwords)")
    
    for vocab_size in vocab_sizes:
        model_name = f'sp_model_{vocab_size}'
        print(f"\n  🔧 Training vocab_size={vocab_size}...")
        print(f"     This will create {vocab_size} different text pieces")
        print(f"     Smaller vocab = more pieces per word (fine-grained)")
        print(f"     Larger vocab = fewer pieces per word (coarse-grained)")
        
        # Train BPE model with SentencePiece
        # This is the actual BPE algorithm execution
        smp.SentencePieceTrainer.train(
            input=train_file,              # Training data (train.txt)
            model_prefix=model_name,       # Model name to save
            vocab_size=vocab_size,         # Vocabulary size (number of pieces)
            model_type='bpe',              # Use BPE algorithm
            character_coverage=1.0,        # Cover all characters (0.9995 for English)
            unk_surface='<unk>'           # Unknown word representation
        )
        print(f"  ✅ Model saved: {model_name}.model & {model_name}.vocab")
        print(f"     This model knows how to split any text into {vocab_size} pieces")


def evaluate_tokenization(test_file: str, vocab_sizes: List[int]) -> List[dict]:
    """
    Tokenize test data with each model and calculate average token count
    
    Purpose: Test how trained models actually split sentences
    Evaluation metric: Average number of tokens per line
    """
    print("📊 Evaluating tokenization...")
    print("   Goal: Test how each model splits unseen Shakespeare text")
    
    # Load test data (1% of data not used in training)
    with open(test_file, 'r', encoding='utf-8') as f:
        test_lines = [line.strip() for line in f.readlines() if line.strip()]
    
    print(f"   Testing on {len(test_lines)} unseen lines")
    
    results = []
    
    for vocab_size in vocab_sizes:
        model_file = f'sp_model_{vocab_size}.model'
        print(f"\n  🧪 Testing model with vocab_size={vocab_size}")
        
        # Load trained SentencePiece model
        sp = smp.SentencePieceProcessor()
        sp.load(model_file)
        print(f"     Model loaded: {model_file}")
        
        # Tokenize all lines and collect statistics
        total_tokens = 0
        total_lines = len(test_lines)
        
        print(f"     Processing {total_lines} test lines...")
        for i, line in enumerate(test_lines):
            # This is the actual splitting process
            # Example: "To be or not to be" → ["To", "be", "or", "not", "to", "be"]
            tokens = sp.encode_as_pieces(line)
            total_tokens += len(tokens)
            
            # Show examples for first 3 lines only
            if i < 3:
                print(f"       Example {i+1}: '{line[:50]}...' → {len(tokens)} tokens")
                print(f"                   Tokens: {tokens[:10]}...")  # Show only first 10
        
        # Calculate average token count
        avg_tokens = total_tokens / total_lines if total_lines > 0 else 0
        
        # Save results to dictionary
        result = {
            'vocab_size': vocab_size,
            'total_lines': total_lines,
            'total_tokens': total_tokens,
            'avg_tokens_per_line': round(avg_tokens, 2)
        }
        
        results.append(result)
        print(f"  📈 vocab_size={vocab_size}: avg={avg_tokens:.2f} tokens/line")
        print(f"     Total tokens generated: {total_tokens}")
    
    return results


def save_results(results: List[dict], output_file: str = 'results.csv'):
    """
    Save results to CSV file
    """
    print(f"💾 Saving results to {output_file}...")
    
    fieldnames = ['vocab_size', 'total_lines', 'total_tokens', 'avg_tokens_per_line']
    
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)
    
    print("✅ Results saved!")


def print_summary(results: List[dict]):
    """
    Display summary of results
    
    Purpose: Show comparison results of 3 models in an understandable way
    Interpretation: Judge segmentation granularity from token count
    """
    print("\n" + "="*50)
    print("📈 SUMMARY: Tokenization Results")
    print("="*50)
    print("   Comparing how different vocab sizes affect text splitting")
    
    print(f"\n{'Vocab Size':<12} {'Avg Tokens':<12} {'Interpretation'}")
    print("-" * 40)
    
    for result in results:
        vocab_size = result['vocab_size']
        avg_tokens = result['avg_tokens_per_line']
        
        # Determine segmentation granularity
        if avg_tokens > 20:
            interpretation = "Fine-grained"    # Fine segmentation (many pieces)
        elif avg_tokens > 10:
            interpretation = "Medium"          # Medium segmentation
        else:
            interpretation = "Coarse-grained"  # Coarse segmentation (few pieces)
        
        print(f"{vocab_size:<12} {avg_tokens:<12} {interpretation}")
    
    print("\n💡 Key Findings:")
    print("- Smaller vocab_size → More tokens per sentence (fine-grained)")
    print("  Example: 'unhappy' → ['un', 'h', 'app', 'y'] (4 tokens)")
    print("- Larger vocab_size → Fewer tokens per sentence (coarse-grained)")  
    print("  Example: 'unhappy' → ['unhappy'] (1 token)")
    print("\n🎯 Practical Impact:")
    print("- Fine-grained: Better for rare words, longer sequences")
    print("- Coarse-grained: Better for common words, shorter sequences")


def main():
    """
    Main processing
    
    Execution flow:
    1. Split Shakespeare text 99:1
    2. Train BPE models with 3 vocabulary sizes (100, 500, 1000)
    3. Split test data with each model and calculate average token count
    4. Save results to CSV and display comparison
    """
    print("🎭 Shakespeare Tokenization Experiment - Task 1 Minimal Setup")
    print("=" * 60)
    print("Purpose: Compare how different vocab sizes affect text segmentation")
    print("Method: Train 3 BPE models and measure average tokens per line")
    
    # Experimental settings
    input_file = 'shakes.txt'
    vocab_sizes = [100, 500, 1000]  # Vocabulary sizes suitable for small data
    
    print(f"\n🔧 Experiment Configuration:")
    print(f"   Input file: {input_file}")
    print(f"   Vocab sizes to test: {vocab_sizes}")
    print(f"   Split ratio: 99% train, 1% test")
    print(f"   Algorithm: Byte Pair Encoding (BPE)")
    
    # Check file existence
    if not os.path.exists(input_file):
        print(f"\n❌ Error: {input_file} not found!")
        print("Please place your Shakespeare text file as 'shakes.txt'")
        print("You can download it from: https://www.gutenberg.org/ebooks/100")
        return
    
    try:
        print(f"\n🚀 Starting experiment...")
        
        # Step 1: Data splitting (ML basics - separate training and evaluation data)
        print(f"\n{'='*20} STEP 1: Data Splitting {'='*20}")
        train_file, test_file = split_data(input_file)
        
        # Step 2: Model training (Learn optimal splitting with BPE algorithm)
        print(f"\n{'='*20} STEP 2: Model Training {'='*20}")
        train_sentencepiece_models(train_file, vocab_sizes)
        
        # Step 3: Evaluation (Measure trained model performance)
        print(f"\n{'='*20} STEP 3: Evaluation {'='*20}")
        results = evaluate_tokenization(test_file, vocab_sizes)
        
        # Step 4: Save results (Record experimental results)
        print(f"\n{'='*20} STEP 4: Save Results {'='*20}")
        save_results(results)
        
        # Step 5: Summary display (Interpret and analyze results)
        print(f"\n{'='*20} STEP 5: Analysis {'='*20}")
        print_summary(results)
        
        print(f"\n🎉 Task 1 Complete!")
        print(f"📁 Generated files:")
        print(f"   - train.txt, test.txt (split data)")
        print(f"   - sp_model_*.model (trained models)")  
        print(f"   - results.csv (experimental results)")
        print(f"\n📊 Check results.csv for detailed output and analysis.")
        
    except Exception as e:
        print(f"❌ Error occurred: {e}")
        print("Please check your input file and try again.")
        print("Common issues:")
        print("- Missing shakes.txt file")
        print("- File encoding problems (try UTF-8)")
        print("- Insufficient disk space for model files")


if __name__ == "__main__":
    # Set seed for reproducibility
    random.seed(42)
    main() 
